In 2020 I was taking  a masters degree in computer science at the university of Stavanger, I remember working in a final project involving the creation of a language model by analyzing a corpus of data and probability distributions. 
Back in those years text analysis and data mining were mature enough that i had already acces to python packages that would perform Natural Language processing in batch processes, which require intense computing power to analyze 20 GB of text and divide such data into tokens, 
- Sentiment analysis: the dissection of data (text, voice, etc) in order to determine whether it’s positive, neutral, or negative.
- Named Entity Recognition: identifying and classifying named entities in text into predefined categories.
- Text Summary: generating a summary of a given text.
- Topic Modeling: identifying topics present in a given text.
- Text Classification: classifying text into predefined categories.
- Keyword Extraction: identifying and extracting important words or phrases from a given text.
- Lemmatization and Stemming: reducing words to their base or root form.
I remember trying to analyze in my project the sentiment of determined text that I could search from my language model to give some king of meaning or bias information in the corpus that had been extracted. 

It was indeed a difficult approach since i did not have enough computational resources to run the models with the current technology, then after googling for a bit longer I found that not so long ago google had created a pseudo-trained language model through a technology called "BERT" I realized such framework could actually give me a boost in the project I was doing, it had been trained using deep learning and extensive resources to give a pre-trained model capable already of solving the problem I was looking for. I just needed to feed it with my corpus and move straight into research. 